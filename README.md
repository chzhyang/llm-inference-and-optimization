# llm-inference-and-optimization
Full-stack LLM inference service based on open-source LLM(such as Llama2, ChatGLM2, etc.)
- Inference server providing text generation, chat completion, framework hot switch, RAG, agent, etc
- Support multiple models, inference frameworks and quantizations
- Performance collection
- One-click deployment
- WebUI for chat and RAG
- 
- [Optimized server for LLM inference](./llm-inference-server/README.md)
- [Optimized LLM inference on multiple AI frameworks](./llm-framework-inference/)
